{"description": "In LangChain through 0.0.131, the `LLMMathChain` chain allows prompt injection attacks that can execute arbitrary code via the Python `exec()` method.", "methods": ["Used Python `exec()` method for code injection.", "Executed arbitrary code via prompt injection attacks."]}